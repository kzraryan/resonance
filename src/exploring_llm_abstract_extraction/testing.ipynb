{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Hi! I'm DeepSeek-R1, an AI assistant independently developed by the Chinese company DeepSeek Inc. For detailed information about models and products, please refer to the official documentation.\n",
      "</think>\n",
      "\n",
      "Hi! I'm DeepSeek-R1, an AI assistant independently developed by the Chinese company DeepSeek Inc. For detailed information about models and products, please refer to the official documentation.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='deepseek-r1:8b', messages=[{'role': 'user', 'content': 'what is your max context length'}])\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_code = open('abstracts.txt', 'r', errors='ignore')\n",
    "code = file_code.read()\n",
    "\n",
    "response = ollama.chat(model = 'deepseek-r1:32b', messages=[{'role': 'user', 'content': '''Look at the paper abstract and give me whatever information you can find based on the following categories and return a json formated string: \n",
    "Research category/domain\n",
    "Which LLM was used, if any.\n",
    "What Datasets: MNIST or wikipedia\n",
    "Type of data: Images, Videos, text, geodata, mixed.\n",
    "Machine Learning? Deep Learning? Statistical Learning?\n",
    "Specific algorithms: Random Forest, XGBoost, LSTM?\n",
    "Which languages: Mostly all are python i guess\n",
    "Which libraries: Scikit, pytorch, etc...\n",
    "*Funding for research: NSF, NIH, NASA\n",
    "Cross disciplinary stuff (\"ML applications for drug discovery\" etc)...\n",
    "    - Not sure if this'd work...\n",
    "Timeline: 5 year long, vs 1 year long or 10 year long research or something...\n",
    "*Dataset size: 1000 data points vs 20,000 patients or something\n",
    "\n",
    "*Problem type - Classification, regressionng, time-series forecasting, anomaly detection, simulation\n",
    "*data-type - tabular, image, text, genomic sequences, spatial, time-series\n",
    "*Ethical considerations: Bias detection, privacy, explainability, Fairness in AI models\n",
    "\n",
    "Summary of innovation? few words or a sentence?\n",
    "\n",
    "*Type of study: Observation vs experimental, prospective vs retrospective.\n",
    "*Code and reproducibility: (Useable GitHub links, computation power needed)\n",
    "Important citations\n",
    "Benchmarking (comparing against prior methods)\n",
    "                                                             ''' + code}])\n",
    "\n",
    "with open (\"reabstract.txt\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ollama' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241m.\u001b[39mchat(model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama3.2:latest\u001b[39m\u001b[38;5;124m'\u001b[39m, messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhow does your memory work. Do you know what question I asked you prior to this one?\u001b[39m\u001b[38;5;124m'\u001b[39m}])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ollama' is not defined"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model = 'llama3.2:latest', messages=[{'role': 'user', 'content': 'how does your memory work. Do you know what question I asked you prior to this one?'}])\n",
    "\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
